"""
æ¢¯åº¦æ£€éªŒæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯å²­å›å½’çš„æŸå¤±å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—æ˜¯å¦æ­£ç¡®
"""

import numpy as np
from start_code import (
    compute_regularized_square_loss,
    compute_regularized_square_loss_gradient,
    grad_checker
)


def test_gradient():
    """æµ‹è¯•æ¢¯åº¦è®¡ç®—çš„æ­£ç¡®æ€§"""
    
    print("=" * 70)
    print("å²­å›å½’æ¢¯åº¦æ£€éªŒæµ‹è¯•")
    print("=" * 70)
    
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    np.random.seed(42)
    
    # æµ‹è¯•æ¡ˆä¾‹1ï¼šå°è§„æ¨¡æ•°æ®
    print("\nã€æµ‹è¯•1ã€‘å°è§„æ¨¡æ•°æ® (10ä¸ªæ ·æœ¬, 5ä¸ªç‰¹å¾)")
    print("-" * 70)
    
    X1 = np.random.randn(10, 5)
    y1 = np.random.randn(10)
    theta1 = np.random.randn(5)
    lambda1 = 0.1
    
    # è®¡ç®—æŸå¤±å€¼
    loss1 = compute_regularized_square_loss(X1, y1, theta1, lambda1)
    print(f"ç›®æ ‡å‡½æ•°å€¼ J(Î¸): {loss1:.6f}")
    
    # è®¡ç®—æ¢¯åº¦
    grad1 = compute_regularized_square_loss_gradient(X1, y1, theta1, lambda1)
    print(f"æ¢¯åº¦å‘é‡: {grad1}")
    print(f"æ¢¯åº¦èŒƒæ•°: {np.linalg.norm(grad1):.6f}")
    
    # æ¢¯åº¦æ£€éªŒ
    is_correct1 = grad_checker(X1, y1, theta1, lambda1)
    result1 = "âœ“ é€šè¿‡" if is_correct1 else "âœ— å¤±è´¥"
    print(f"\næ¢¯åº¦æ£€éªŒç»“æœ: {result1}")
    
    # æµ‹è¯•æ¡ˆä¾‹2ï¼šä¸­ç­‰è§„æ¨¡æ•°æ®
    print("\n" + "=" * 70)
    print("\nã€æµ‹è¯•2ã€‘ä¸­ç­‰è§„æ¨¡æ•°æ® (50ä¸ªæ ·æœ¬, 10ä¸ªç‰¹å¾)")
    print("-" * 70)
    
    X2 = np.random.randn(50, 10)
    y2 = np.random.randn(50)
    theta2 = np.random.randn(10)
    lambda2 = 0.5
    
    loss2 = compute_regularized_square_loss(X2, y2, theta2, lambda2)
    print(f"ç›®æ ‡å‡½æ•°å€¼ J(Î¸): {loss2:.6f}")
    
    grad2 = compute_regularized_square_loss_gradient(X2, y2, theta2, lambda2)
    print(f"æ¢¯åº¦èŒƒæ•°: {np.linalg.norm(grad2):.6f}")
    
    is_correct2 = grad_checker(X2, y2, theta2, lambda2)
    result2 = "âœ“ é€šè¿‡" if is_correct2 else "âœ— å¤±è´¥"
    print(f"\næ¢¯åº¦æ£€éªŒç»“æœ: {result2}")
    
    # æµ‹è¯•æ¡ˆä¾‹3ï¼šä¸åŒçš„æ­£åˆ™åŒ–ç³»æ•°
    print("\n" + "=" * 70)
    print("\nã€æµ‹è¯•3ã€‘ä¸åŒæ­£åˆ™åŒ–ç³»æ•°çš„å½±å“")
    print("-" * 70)
    
    X3 = np.random.randn(20, 8)
    y3 = np.random.randn(20)
    theta3 = np.random.randn(8)
    
    lambdas = [0.0, 0.01, 0.1, 1.0, 10.0]
    all_passed = True
    
    print(f"{'Î»':>8} {'J(Î¸)':>12} {'||âˆ‡J||':>12} {'æ¢¯åº¦æ£€éªŒ':>12}")
    print("-" * 70)
    
    for lam in lambdas:
        loss = compute_regularized_square_loss(X3, y3, theta3, lam)
        grad = compute_regularized_square_loss_gradient(X3, y3, theta3, lam)
        grad_norm = np.linalg.norm(grad)
        is_correct = grad_checker(X3, y3, theta3, lam)
        result = "âœ“ é€šè¿‡" if is_correct else "âœ— å¤±è´¥"
        
        print(f"{lam:8.2f} {loss:12.6f} {grad_norm:12.6f} {result:>12}")
        
        if not is_correct:
            all_passed = False
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("\nã€æµ‹è¯•æ€»ç»“ã€‘")
    print("-" * 70)
    
    test_results = [
        ("æµ‹è¯•1 (å°è§„æ¨¡æ•°æ®)", is_correct1),
        ("æµ‹è¯•2 (ä¸­ç­‰è§„æ¨¡æ•°æ®)", is_correct2),
        ("æµ‹è¯•3 (ä¸åŒæ­£åˆ™åŒ–ç³»æ•°)", all_passed)
    ]
    
    all_tests_passed = all(result for _, result in test_results)
    
    for test_name, result in test_results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name:.<50} {status}")
    
    print("\n" + "=" * 70)
    
    if all_tests_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¢¯åº¦è®¡ç®—å®ç°æ­£ç¡®ï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ï¼")
    
    print("=" * 70 + "\n")
    
    return all_tests_passed


def manual_gradient_check():
    """æ‰‹åŠ¨å¯¹æ¯”è§£ææ¢¯åº¦å’Œæ•°å€¼æ¢¯åº¦ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    
    print("\n" + "=" * 70)
    print("ã€æ‰‹åŠ¨æ¢¯åº¦æ£€éªŒã€‘è§£ææ¢¯åº¦ vs æ•°å€¼æ¢¯åº¦")
    print("=" * 70 + "\n")
    
    np.random.seed(42)
    X = np.random.randn(5, 3)
    y = np.random.randn(5)
    theta = np.random.randn(3)
    lambda_reg = 0.1
    epsilon = 0.01
    
    # è®¡ç®—è§£ææ¢¯åº¦
    grad_analytical = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)
    
    # æ‰‹åŠ¨è®¡ç®—æ•°å€¼æ¢¯åº¦
    grad_numerical = np.zeros_like(theta)
    for i in range(len(theta)):
        theta_plus = theta.copy()
        theta_minus = theta.copy()
        
        theta_plus[i] += epsilon
        theta_minus[i] -= epsilon
        
        J_plus = compute_regularized_square_loss(X, y, theta_plus, lambda_reg)
        J_minus = compute_regularized_square_loss(X, y, theta_minus, lambda_reg)
        
        grad_numerical[i] = (J_plus - J_minus) / (2 * epsilon)
    
    # é€ä¸ªåˆ†é‡æ¯”è¾ƒ
    print(f"{'å‚æ•°':>8} {'è§£ææ¢¯åº¦':>15} {'æ•°å€¼æ¢¯åº¦':>15} {'å·®å¼‚':>15}")
    print("-" * 70)
    
    for i in range(len(theta)):
        diff = abs(grad_analytical[i] - grad_numerical[i])
        print(f"Î¸[{i}]    {grad_analytical[i]:15.8f} {grad_numerical[i]:15.8f} {diff:15.10f}")
    
    total_diff = np.linalg.norm(grad_analytical - grad_numerical)
    print("-" * 70)
    print(f"æ€»ä½“å·®å¼‚ (æ¬§æ°è·ç¦»): {total_diff:.10f}")
    
    if total_diff < 1e-4:
        print("ç»“æœ: âœ“ æ¢¯åº¦è®¡ç®—æ­£ç¡®\n")
    else:
        print("ç»“æœ: âœ— æ¢¯åº¦è®¡ç®—å¯èƒ½æœ‰è¯¯\n")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_gradient()
    
    # è¿è¡Œæ‰‹åŠ¨æ£€éªŒï¼ˆå¯é€‰ï¼Œç”¨äºè¯¦ç»†è°ƒè¯•ï¼‰
    print("\næ˜¯å¦è¿è¡Œæ‰‹åŠ¨æ¢¯åº¦æ£€éªŒä»¥æŸ¥çœ‹è¯¦ç»†å¯¹æ¯”ï¼Ÿ(y/n): ", end="")
    try:
        choice = input().strip().lower()
        if choice == 'y':
            manual_gradient_check()
    except:
        # å¦‚æœä¸æ˜¯äº¤äº’å¼ç¯å¢ƒï¼Œç›´æ¥è¿è¡Œ
        manual_gradient_check()

