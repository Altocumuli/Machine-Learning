# ä½œä¸š1:çº¿æ€§æ¨¡å‹å’Œæ”¯æŒå‘é‡æœº

**å§“å**: æ¨æ†¬æ™—  
**å­¦å·**: 2022010808

---

## çº¿æ€§æ¨¡å‹ä¸æ¢¯åº¦ä¸‹é™

### 2.1 ç‰¹å¾å½’ä¸€åŒ–

åœ¨å®é™…ä»»åŠ¡ä¸­ï¼Œè‹¥å„ç»´ç‰¹å¾çš„é‡çº§å·®å¼‚è¾ƒå¤§ï¼Œæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›ä¼šæ˜¾è‘—å˜æ…¢ï¼›åŒæ—¶ï¼Œåœ¨ä½¿ç”¨æ­£åˆ™åŒ–æ—¶ï¼Œé‡çº§è¾ƒå¤§çš„ç‰¹å¾å¯¹æ­£åˆ™é¡¹å½±å“æ›´å¼ºã€‚å› æ­¤éœ€è¦è¿›è¡Œç‰¹å¾å½’ä¸€åŒ–ã€‚å¸¸ç”¨åšæ³•æ˜¯åœ¨è®­ç»ƒé›†ä¸Šå¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œä»¿å°„å˜æ¢ï¼Œå°†å…¶æ˜ å°„åˆ°åŒºé—´[0,1]ï¼›å¹¶å¯¹æµ‹è¯•é›†æ–½åŠ ä¸è®­ç»ƒé›†ä¸€è‡´çš„å˜æ¢ã€‚

#### 2.1.1 è¡¥å…¨å‡½æ•°split_dataï¼Œå°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†

**å®ç°æ€è·¯**ï¼š
- æ ¹æ®`split_size`ä¸­çš„æ¯”ä¾‹ï¼Œä¾æ¬¡è®¡ç®—æ¯ä¸ªåˆ’åˆ†çš„æ ·æœ¬æ•°é‡
- ä½¿ç”¨ç´¢å¼•åˆ‡ç‰‡å°†æ•°æ®é›†åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†
- å¤„ç†æœ€åä¸€ä¸ªåˆ’åˆ†ä»¥ç¡®ä¿åŒ…å«æ‰€æœ‰å‰©ä½™æ ·æœ¬

**ä»£ç å®ç°**ï¼š
```python
start_idx = 0
for ratio in split_size:
    # è®¡ç®—å½“å‰åˆ’åˆ†çš„æ ·æœ¬æ•°é‡
    split_num = int(num_instances * ratio)
    end_idx = start_idx + split_num
    
    # å¤„ç†æœ€åä¸€ä¸ªåˆ’åˆ†ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å‰©ä½™æ ·æœ¬
    if end_idx > num_instances or ratio == split_size[-1]:
        end_idx = num_instances
    
    # åˆ’åˆ†æ•°æ®
    X_list.append(X[start_idx:end_idx])
    y_list.append(y[start_idx:end_idx])
    
    start_idx = end_idx

return X_list, y_list
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ç´¯ç§¯ç´¢å¼•æ–¹å¼è¿›è¡Œæ•°æ®åˆ’åˆ†
- æœ€åä¸€ä¸ªåˆ’åˆ†éœ€è¦åŒ…å«æ‰€æœ‰å‰©ä½™æ ·æœ¬ï¼Œé¿å…å› æµ®ç‚¹æ•°è®¡ç®—å¯¼è‡´çš„æ ·æœ¬ä¸¢å¤±

#### 2.1.2 è¡¥å…¨å‡½æ•°feature_normalizationï¼Œå®ç°ç‰¹å¾å½’ä¸€åŒ–

**å®ç°æ€è·¯**ï¼š
- åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—æ¯ä¸ªç‰¹å¾çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
- ä½¿ç”¨Min-Maxå½’ä¸€åŒ–å…¬å¼: \( x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}} \)
- å¯¹æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„æœ€å°å€¼å’Œæœ€å¤§å€¼è¿›è¡Œç›¸åŒçš„å˜æ¢

**ä»£ç å®ç°**ï¼š
```python
# ä»è®­ç»ƒé›†è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
train_min = np.min(train, axis=0)
train_max = np.max(train, axis=0)

# é¿å…é™¤é›¶é”™è¯¯ï¼šå¦‚æœæœ€å¤§å€¼ç­‰äºæœ€å°å€¼ï¼Œè¯´æ˜è¯¥ç‰¹å¾ä¸ºå¸¸æ•°
# æ­¤æ—¶å°†èŒƒå›´è®¾ä¸º1ï¼Œä¿æŒå½’ä¸€åŒ–åçš„å€¼ä¸å˜
range_vals = train_max - train_min
range_vals[range_vals == 0] = 1  # é¿å…é™¤é›¶

# å°†è®­ç»ƒé›†å½’ä¸€åŒ–åˆ° [0, 1]
train_normalized = (train - train_min) / range_vals

# å¯¹æµ‹è¯•é›†ä½¿ç”¨è®­ç»ƒé›†çš„æœ€å°å€¼å’Œæœ€å¤§å€¼è¿›è¡Œç›¸åŒçš„å˜æ¢
test_normalized = (test - train_min) / range_vals

return train_normalized, test_normalized
```

**å…³é”®ç‚¹**ï¼š
1. **ç»Ÿè®¡é‡çš„è®¡ç®—**ï¼šæ‰€æœ‰å½’ä¸€åŒ–å‚æ•°ï¼ˆæœ€å°å€¼ã€æœ€å¤§å€¼ï¼‰éƒ½ä»è®­ç»ƒé›†è®¡ç®—å¾—å‡º
2. **æµ‹è¯•é›†ä¸€è‡´æ€§**ï¼šæµ‹è¯•é›†å¿…é¡»ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡è¿›è¡Œå˜æ¢ï¼Œç¡®ä¿åˆ†å¸ƒä¸€è‡´
3. **è¾¹ç•Œæƒ…å†µå¤„ç†**ï¼šå½“æŸä¸ªç‰¹å¾çš„æœ€å¤§å€¼ç­‰äºæœ€å°å€¼æ—¶ï¼ˆå¸¸æ•°ç‰¹å¾ï¼‰ï¼Œè®¾ç½®èŒƒå›´ä¸º1ä»¥é¿å…é™¤é›¶é”™è¯¯
4. **å‘é‡åŒ–æ“ä½œ**ï¼šä½¿ç”¨numpyçš„å¹¿æ’­æœºåˆ¶ï¼Œå¯¹æ‰€æœ‰ç‰¹å¾åŒæ—¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œæé«˜æ•ˆç‡

**ç†è®ºä¾æ®**ï¼š
- Min-Maxå½’ä¸€åŒ–å°†ç‰¹å¾çº¿æ€§æ˜ å°„åˆ°[0,1]åŒºé—´
- é€šè¿‡ä»¿å°„å˜æ¢ \( x' = \frac{x - min}{max - min} \) å®ç°
- ä¿è¯æ‰€æœ‰ç‰¹å¾å¤„äºç›¸åŒé‡çº§ï¼Œæœ‰åˆ©äºæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›
- åœ¨æ­£åˆ™åŒ–æ—¶ï¼Œé¿å…é‡çº§å¤§çš„ç‰¹å¾ä¸»å¯¼æ­£åˆ™é¡¹

---

### 2.2 ç›®æ ‡å‡½æ•°ä¸æ¢¯åº¦

å²­å›å½’(Ridge Regression)æ˜¯åœ¨çº¿æ€§å›å½’çš„åŸºç¡€ä¸ŠåŠ å…¥L2æ­£åˆ™åŒ–ï¼Œç›®çš„æ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

#### 2.2.1 å†™å‡º\( J(\theta) \)çš„çŸ©é˜µå½¢å¼è¡¨è¾¾å¼

**å·²çŸ¥æ¡ä»¶**ï¼š
- è®¾è®¡çŸ©é˜µï¼š\( X \in \mathbb{R}^{m \times (d+1)} \)ï¼Œå…¶ä¸­ç¬¬ \( i \) è¡Œä¸º \( x_i^\top \)
- æ ‡ç­¾å‘é‡ï¼š\( y \in \mathbb{R}^m \)
- å‚æ•°å‘é‡ï¼š\( \theta \in \mathbb{R}^{d+1} \)
- æ­£åˆ™åŒ–ç³»æ•°ï¼š\( \lambda > 0 \)

**ç›®æ ‡å‡½æ•°**ï¼š

å²­å›å½’çš„ç›®æ ‡å‡½æ•°ä¸ºï¼š
\[
J(\theta) = \frac{1}{m} \| X\theta - y \|^2 + \lambda \| \theta \|^2
\]

å±•å¼€ä¸ºçŸ©é˜µå½¢å¼ï¼š
\[
J(\theta) = \frac{1}{m}(X\theta - y)^\top (X\theta - y) + \lambda \theta^\top \theta
\]

å…¶ä¸­ï¼š
- ç¬¬ä¸€é¡¹ \( \frac{1}{m}(X\theta - y)^\top (X\theta - y) \) æ˜¯å‡æ–¹è¯¯å·®æŸå¤±
- ç¬¬äºŒé¡¹ \( \lambda \theta^\top \theta \) æ˜¯L2æ­£åˆ™åŒ–é¡¹

#### 2.2.2 è¡¥å…¨å‡½æ•°compute_regularized_square_lossï¼Œè®¡ç®—\( J(\theta) \)

**å®ç°ç›®æ ‡**ï¼šæ ¹æ® \( J(\theta) = \frac{1}{m}(X\theta - y)^\top (X\theta - y) + \lambda \theta^\top \theta \) å®ç°ç›®æ ‡å‡½æ•°ã€‚

**ä»£ç å®ç°**ï¼š
```python
num_instances = X.shape[0]

# è®¡ç®—é¢„æµ‹å€¼
predictions = np.dot(X, theta)

# è®¡ç®—æ®‹å·®
residuals = predictions - y

# è®¡ç®—å¹³æ–¹æŸå¤±: (1/m) * ||XÎ¸ - y||Â²
square_loss = np.dot(residuals, residuals) / num_instances

# è®¡ç®—æ­£åˆ™åŒ–é¡¹: Î» * ||Î¸||Â²
regularization = lambda_reg * np.dot(theta, theta)

# ç›®æ ‡å‡½æ•° J(Î¸)
loss = square_loss + regularization

return loss
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `np.dot` è¿›è¡Œå‘é‡åŒ–è®¡ç®—

#### 2.2.3 æ¨å¯¼\( J(\theta) \)çš„æ¢¯åº¦

**æ¨å¯¼è¿‡ç¨‹**ï¼š

å¯¹ \( J(\theta) = \frac{1}{m}(X\theta - y)^\top(X\theta - y) + \lambda \theta^\top\theta \) æ±‚æ¢¯åº¦ã€‚

åˆ†åˆ«å¯¹ä¸¤é¡¹æ±‚å¯¼ï¼š

1. **å¯¹å‡æ–¹è¯¯å·®é¡¹æ±‚å¯¼**ï¼š
   \[
   \frac{\partial}{\partial\theta}\left[\frac{1}{m}(X\theta - y)^\top(X\theta - y)\right] = \frac{2}{m}X^\top(X\theta - y)
   \]

2. **å¯¹æ­£åˆ™åŒ–é¡¹æ±‚å¯¼**ï¼š
   \[
   \frac{\partial}{\partial\theta}\left[\lambda\theta^\top\theta\right] = 2\lambda\theta
   \]

**æ¢¯åº¦è¡¨è¾¾å¼**ï¼š
\[
\nabla J(\theta) = \frac{2}{m}X^\top(X\theta - y) + 2\lambda\theta
\]

**æ¢¯åº¦å«ä¹‰**ï¼š
- \( \frac{2}{m}X^\top(X\theta - y) \)ï¼šå‡æ–¹è¯¯å·®é¡¹çš„æ¢¯åº¦ï¼Œå°†æ®‹å·®æŠ•å½±åˆ°ç‰¹å¾ç©ºé—´
- \( 2\lambda\theta \)ï¼šæ­£åˆ™åŒ–é¡¹çš„æ¢¯åº¦ï¼Œä½¿å‚æ•°å‘é›¶æ”¶ç¼©ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- æ¢¯åº¦ä¸‹é™æ›´æ–°ï¼š\( \theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t) \)

#### 2.2.4 è¡¥å…¨å‡½æ•°compute_regularized_square_loss_gradientï¼Œå®ç°\( J(\theta) \)çš„æ¢¯åº¦

**å®ç°ç›®æ ‡**ï¼š \( \nabla J(\theta) = \frac{2}{m}X^\top(X\theta - y) + 2\lambda\theta \)

**ä»£ç å®ç°**ï¼š
```python
num_instances = X.shape[0]

# è®¡ç®—é¢„æµ‹å€¼
predictions = np.dot(X, theta)

# è®¡ç®—æ®‹å·®
residuals = predictions - y

# è®¡ç®—æ¢¯åº¦: (2/m) * X^T(XÎ¸ - y) + 2Î»Î¸
grad = 2 * np.dot(X.T, residuals) / num_instances + 2 * lambda_reg * theta

return grad
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `X.T` è¿›è¡ŒçŸ©é˜µè½¬ç½®
- å‘é‡åŒ–å®ç°ï¼Œæ— éœ€å¾ªç¯

---

### 2.3 æ¢¯åº¦ä¸‹é™

#### 2.3.1 ç”¨æ¢¯åº¦å†™å‡ºç›®æ ‡å‡½æ•°å€¼å˜åŒ–çš„è¿‘ä¼¼è¡¨è¾¾å¼

**é—®é¢˜**ï¼šåœ¨æœ€å°åŒ– \( J(\theta) \) æ—¶ï¼Œè€ƒè™‘ä»å½“å‰å‚æ•° \( \theta \) æ²¿æ–¹å‘ \( h \in \mathbb{R}^{d+1} \) å‰è¿›ä¸€æ­¥è‡³ \( \theta + \eta h \)ï¼Œå…¶ä¸­ \( \eta > 0 \) ä¸ºæ­¥é•¿ã€‚è¯·ç”¨æ¢¯åº¦å†™å‡ºç›®æ ‡å‡½æ•°å€¼å˜åŒ–çš„è¿‘ä¼¼è¡¨è¾¾å¼ \( J(\theta + \eta h) - J(\theta) \)ï¼Œæ€è€ƒ \( h \) ä¸ºå“ªä¸€å‰è¿›æ–¹å‘æ—¶ç›®æ ‡å‡½æ•°ä¸‹é™æœ€å¿«ï¼Œå¹¶æ®æ­¤å†™å‡ºæ¢¯åº¦ä¸‹é™ä¸­æ›´æ–° \( \theta \) çš„è¡¨è¾¾å¼ã€‚

**1. ç›®æ ‡å‡½æ•°å€¼å˜åŒ–çš„è¿‘ä¼¼è¡¨è¾¾å¼**

å¯¹å…‰æ»‘å‡½æ•° \( J(\theta) \) åœ¨ç‚¹ \( \theta \) å¤„è¿›è¡Œä¸€é˜¶æ³°å‹’å±•å¼€ï¼š
\[
J(\theta + \eta h) \approx J(\theta) + \eta \nabla J(\theta)^\top h
\]

å› æ­¤ï¼Œç›®æ ‡å‡½æ•°å€¼çš„å˜åŒ–è¿‘ä¼¼ä¸ºï¼š
\[
J(\theta + \eta h) - J(\theta) \approx \eta \nabla J(\theta)^\top h
\]

**2. ä¸‹é™æœ€å¿«çš„æ–¹å‘**

ä¸ºä½¿ç›®æ ‡å‡½æ•°ä¸‹é™ï¼Œéœ€è¦ \( \nabla J(\theta)^\top h < 0 \)ã€‚

è¦ä½¿ä¸‹é™æœ€å¿«ï¼Œåº”ä½¿ \( \nabla J(\theta)^\top h \) æœ€å°ã€‚ç”±æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼ï¼š
\[
\nabla J(\theta)^\top h \geq -\|\nabla J(\theta)\| \cdot \|h\|
\]

å½“ä¸”ä»…å½“ \( h = -\nabla J(\theta) \) æ—¶ç­‰å·æˆç«‹ï¼Œå³**è´Ÿæ¢¯åº¦æ–¹å‘**æ˜¯ç›®æ ‡å‡½æ•°ä¸‹é™æœ€å¿«çš„æ–¹å‘ã€‚

**3. æ¢¯åº¦ä¸‹é™æ›´æ–°è§„åˆ™**

æ¢¯åº¦ä¸‹é™æ³•çš„å‚æ•°æ›´æ–°å…¬å¼ä¸ºï¼š
\[
\theta := \theta - \eta \nabla J(\theta)
\]

å¯¹äºå²­å›å½’ï¼Œæ¢¯åº¦ä¸º \( \nabla J(\theta) = \frac{2}{m} X^\top (X\theta - y) + 2\lambda \theta \)ï¼Œå› æ­¤æ›´æ–°è§„åˆ™ä¸ºï¼š
\[
\theta := \theta - \eta \left[\frac{2}{m} X^\top (X\theta - y) + 2\lambda \theta\right]
\]

å…¶ä¸­ \( \eta \) æ˜¯å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰ã€‚

#### 2.3.2 è¡¥å…¨å‡½æ•°grad_descentï¼Œå®ç°å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ç®—æ³•

**ç®—æ³•æµç¨‹**ï¼š

å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBatch Gradient Descentï¼‰åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ã€‚

**ä»£ç å®ç°**ï¼š
```python
for i in range(num_iter):
    # è®¡ç®—å½“å‰æŸå¤±
    loss_hist[i] = compute_regularized_square_loss(X, y, theta, lambda_reg)
    
    # è®¡ç®—æ¢¯åº¦
    grad = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)
    
    # æ¢¯åº¦æ£€æŸ¥
    if check_gradient:
        if not grad_checker(X, y, theta, lambda_reg):
            print(f"è­¦å‘Šï¼šç¬¬{i}æ¬¡è¿­ä»£æ¢¯åº¦æ£€æŸ¥æœªé€šè¿‡")
    
    # æ›´æ–°å‚æ•°ï¼šÎ¸ := Î¸ - Î± * âˆ‡J(Î¸)
    theta = theta - alpha * grad
    
    # ä¿å­˜å‚æ•°å†å²
    theta_hist[i + 1] = theta

return theta_hist, loss_hist
```

**å…³é”®ç‚¹**ï¼š
- æ¯æ¬¡è¿­ä»£ä½¿ç”¨**å…¨éƒ¨è®­ç»ƒæ•°æ®**è®¡ç®—æ¢¯åº¦
- æŒ‰ç…§æ¢¯åº¦ä¸‹é™å…¬å¼ \( \theta := \theta - \alpha \nabla J(\theta) \) æ›´æ–°å‚æ•°
- è®°å½•æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼å’Œå‚æ•°å€¼ï¼Œç”¨äºåç»­åˆ†æ
- å¯é€‰æ‹©æ€§åœ°è¿›è¡Œæ¢¯åº¦æ£€æŸ¥ä»¥éªŒè¯æ¢¯åº¦è®¡ç®—çš„æ­£ç¡®æ€§

**ç®—æ³•ç‰¹ç‚¹**ï¼š
- **ä¼˜ç‚¹**ï¼šæ¯æ¬¡è¿­ä»£æ–¹å‘å‡†ç¡®ï¼Œæ”¶æ•›ç¨³å®š
- **ç¼ºç‚¹**ï¼šæ•°æ®é‡å¤§æ—¶è®¡ç®—ä»£ä»·é«˜ï¼Œæ¯æ¬¡è¿­ä»£éœ€è¦éå†å…¨éƒ¨æ•°æ®
- **é€‚ç”¨åœºæ™¯**ï¼šä¸­å°è§„æ¨¡æ•°æ®é›†ï¼Œæˆ–éœ€è¦ç²¾ç¡®æ”¶æ•›çš„åœºæ™¯

#### 2.3.3 æ­¥é•¿é€‰æ‹©å®éªŒ

**å®éªŒç›®çš„**ï¼šç ”ç©¶ä¸åŒæ­¥é•¿å¯¹æ¢¯åº¦ä¸‹é™æ”¶æ•›æ€§èƒ½çš„å½±å“ã€‚

**å®éªŒè®¾ç½®**ï¼š
- å›ºå®šæ­£åˆ™åŒ–ç³»æ•° \( \lambda = 0 \)
- æµ‹è¯•æ­¥é•¿ï¼š\( \eta \in \{0.01, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.5\} \)
- è¿­ä»£æ¬¡æ•°ï¼š1000æ¬¡
- è§‚å¯Ÿç›®æ ‡å‡½æ•° \( J(\theta) \) éšè¿­ä»£æ¬¡æ•°çš„å˜åŒ–
- æ•°æ®é›†ï¼š200ä¸ªæ ·æœ¬ï¼Œ48ä¸ªç‰¹å¾ï¼ˆè®­ç»ƒé›†160ï¼Œæµ‹è¯•é›†40ï¼‰

**å®éªŒè„šæœ¬**ï¼š
```bash
python experiment_step_size.py
```

**å®éªŒç»“æœ**ï¼š

è¿è¡Œå®éªŒè„šæœ¬åï¼Œç”Ÿæˆä¸åŒæ­¥é•¿ä¸‹çš„æ”¶æ•›æ›²çº¿å¯¹æ¯”å›¾ï¼š

![æ­¥é•¿å¯¹æ¯”å®éªŒç»“æœ](sgd/step_size_comparison.png)

**å®é™…ç»“æœåˆ†æ**ï¼š

| æ­¥é•¿ \( \eta \) | åˆå§‹æŸå¤± | æœ€ç»ˆæŸå¤± | æ”¶æ•›æƒ…å†µ | æŸå¤±ä¸‹é™ | ç‰¹ç‚¹ |
|:---:|:---:|:---:|:---:|:---:|:---|
| 0.01 | 6.897 | 2.839 | âœ“ æ”¶æ•› | 58.84% | æ­¥é•¿è¾ƒå°ï¼Œæ”¶æ•›ç¨³å®šä½†é€Ÿåº¦æ…¢ |
| 0.05 | 6.897 | 2.306 | âœ“ æ”¶æ•› | 66.57% | **æœ€ä¼˜æ­¥é•¿**ï¼Œæ”¶æ•›ç¨³å®šä¸”ä¸‹é™å……åˆ† |
| 0.06 | 6.897 | 1.14Ã—10Â¹Â²Â¹ | âœ— å‘æ•£ | - | åˆšè¶…è¿‡ä¸´ç•Œå€¼ï¼Œæ•°å€¼çˆ†ç‚¸ |
| 0.07 | 6.897 | NaN | âœ— å‘æ•£ | - | æ­¥é•¿è¿‡å¤§ï¼Œä¸¥é‡å‘æ•£ |
| 0.08 | 6.897 | NaN | âœ— å‘æ•£ | - | æ­¥é•¿è¿‡å¤§ï¼Œä¸¥é‡å‘æ•£ |
| 0.09 | 6.897 | NaN | âœ— å‘æ•£ | - | æ­¥é•¿è¿‡å¤§ï¼Œä¸¥é‡å‘æ•£ |
| 0.1 | 6.897 | NaN | âœ— å‘æ•£ | - | æ­¥é•¿è¿‡å¤§ï¼Œä¸¥é‡å‘æ•£ |
| 0.5 | 6.897 | NaN | âœ— å‘æ•£ | - | æ­¥é•¿è¿‡å¤§ï¼Œä¸¥é‡å‘æ•£ |

**ç»“è®º**ï¼š

1. **æ”¶æ•›æœ€å¿«çš„æ­¥é•¿**ï¼š\( \eta = 0.05 \)
   - åœ¨ä¿è¯æ”¶æ•›çš„æ‰€æœ‰æ­¥é•¿ä¸­ï¼Œè¯¥æ­¥é•¿å®ç°äº†æœ€å¤§çš„æŸå¤±ä¸‹é™ï¼ˆ66.57%ï¼‰
   - ç›¸æ¯” \( \eta = 0.01 \)ï¼Œæ”¶æ•›æ›´å……åˆ†ä¸”é€Ÿåº¦æ›´å¿«
   - æ˜¯æœ¬æ•°æ®é›†ä¸Šçš„**æœ€ä¼˜é€‰æ‹©**

2. **å¯¼è‡´å‘æ•£çš„æ­¥é•¿**ï¼š\( \eta \geq 0.06 \)
   - å½“ \( \eta = 0.06 \) æ—¶ï¼Œå‚æ•°å¼€å§‹çˆ†ç‚¸æ€§å¢é•¿ï¼ŒæŸå¤±è¾¾åˆ° \( 10^{121} \) é‡çº§
   - ä» \( \eta = 0.07 \) å¼€å§‹ï¼Œç›´æ¥å¯¼è‡´NaNï¼ˆæ•°å€¼æº¢å‡ºï¼‰
   - é€šè¿‡ç»†åŒ–å®éªŒï¼Œç¡®å®šæ­¥é•¿ä¸Šç•Œéå¸¸æ¥è¿‘ \( \eta_{max} \approx 0.055 \)ï¼ˆä»‹äº0.05å’Œ0.06ä¹‹é—´ï¼‰

**ç†è®ºåˆ†æ**ï¼š

å¯¹äºæ¢¯åº¦ä¸‹é™ï¼Œæ­¥é•¿éœ€è¦æ»¡è¶³ä¸€å®šçš„ä¸Šç•Œæ¡ä»¶ã€‚å¯¹äºå…‰æ»‘ä¸”Lipschitzè¿ç»­çš„ç›®æ ‡å‡½æ•°ï¼Œæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›è¦æ±‚ï¼š
\[
\eta < \frac{2}{L}
\]
å…¶ä¸­ \( L \) æ˜¯ç›®æ ‡å‡½æ•°æ¢¯åº¦çš„Lipschitzå¸¸æ•°ã€‚

åœ¨æœ¬å®éªŒä¸­ï¼š
- æ•°æ®ç»´åº¦è¾ƒé«˜ï¼ˆ48ä¸ªç‰¹å¾ï¼‰ï¼Œä¸”æœªè¿›è¡Œæ­£åˆ™åŒ–ï¼ˆ\( \lambda = 0 \)ï¼‰
- è¿™å¯¼è‡´æ¢¯åº¦çš„Lipschitzå¸¸æ•° \( L \) è¾ƒå¤§ï¼Œæ­¥é•¿ä¸Šç•Œç›¸åº”å˜å°
- é€šè¿‡ç»†åŒ–å®éªŒï¼Œç²¾ç¡®å®šä½æ­¥é•¿ä¸Šç•Œçº¦ä¸º \( \eta_{max} \approx 0.055 \)ï¼ˆä»‹äº0.05å’Œ0.06ä¹‹é—´ï¼‰
- å½“ \( \eta = 0.06 \) æ—¶ï¼Œå‚æ•°æ›´æ–°å·²è¶…è¿‡ä¸´ç•Œç‚¹ï¼ŒæŸå¤±åœ¨å‡ æ¬¡è¿­ä»£åçˆ†ç‚¸è‡³ \( 10^{121} \) é‡çº§
- ä» \( \eta = 0.07 \) å¼€å§‹ï¼Œæ¢¯åº¦çˆ†ç‚¸ï¼ˆoverflowï¼‰å¯¼è‡´ç«‹å³å‡ºç°NaN

**å…³é”®è§‚å¯Ÿ**ï¼š

1. **æ­¥é•¿æ•æ„Ÿæ€§**ï¼š
   - ä»…0.01å’Œ0.05ä¸¤ä¸ªæ­¥é•¿èƒ½å¤Ÿæ”¶æ•›ï¼Œé€‰æ‹©èŒƒå›´æå…¶ç‹­çª„
   - æ­¥é•¿ä»0.05å¢åŠ åˆ°0.06ï¼ˆä»…å¢åŠ 20%ï¼‰ï¼Œç«‹å³ä»æ”¶æ•›å˜ä¸ºå‘æ•£
   - \( \eta = 0.06 \) æ—¶æŸå¤±çˆ†ç‚¸è‡³ \( 10^{121} \)ï¼Œ\( \eta = 0.07 \) æ—¶ç›´æ¥NaN
   - è¯´æ˜è¯¥é—®é¢˜å¯¹æ­¥é•¿çš„é€‰æ‹©**æåº¦æ•æ„Ÿ**ï¼Œæ­¥é•¿ä¸Šç•Œçš„å®¹å¿åº¦å‡ ä¹ä¸ºé›¶

2. **æ”¶æ•›æ›²çº¿ç‰¹å¾**ï¼š
   - \( \eta = 0.01 \)ï¼šæ”¶æ•›å¹³ç¨³ï¼Œä½†1000æ¬¡è¿­ä»£åæŸå¤±ä»ä¸º2.839
   - \( \eta = 0.05 \)ï¼šæ”¶æ•›æ›´å¿«ï¼Œ1000æ¬¡è¿­ä»£åæŸå¤±é™è‡³2.306
   - åœ¨å¯¹æ•°å°ºåº¦å›¾ä¸­ï¼Œä¸¤æ¡æ”¶æ•›æ›²çº¿å‘ˆç°è¿‘ä¼¼æŒ‡æ•°ä¸‹é™è¶‹åŠ¿

3. **å®è·µå¯ç¤º**ï¼š
   - é«˜ç»´æ•°æ®ï¼ˆç‰¹å¾æ•°å¤šï¼‰å¾€å¾€éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡
   - æ— æ­£åˆ™åŒ–æ—¶ï¼Œç›®æ ‡å‡½æ•°çš„æ¡ä»¶æ•°å¯èƒ½å¾ˆå·®ï¼Œéœ€è¦æ›´è°¨æ…é€‰æ‹©æ­¥é•¿
   - å®é™…åº”ç”¨ä¸­å»ºè®®ä»å°æ­¥é•¿å¼€å§‹å°è¯•ï¼Œé€æ­¥å¢å¤§ç›´åˆ°è§‚å¯Ÿåˆ°ä¸ç¨³å®šç°è±¡
   - å¯ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼ŒåˆæœŸä½¿ç”¨è¾ƒå¤§æ­¥é•¿åŠ é€Ÿæ”¶æ•›ï¼ŒåæœŸä½¿ç”¨å°æ­¥é•¿ç²¾ç»†è°ƒæ•´

---

### 2.4 æ¨¡å‹é€‰æ‹©

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©åˆé€‚çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ \( \eta \) å’Œæ­£åˆ™åŒ–ç³»æ•° \( \lambda \)ï¼‰æ¥è·å¾—æœ€ä½³çš„æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†é¿å…åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œå‚æ•°é€‰æ‹©ï¼ˆä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œæ¨¡å‹é€‰æ‹©ã€‚

#### 2.4.1 è¡¥å…¨å‡½æ•°K_fold_split_dataï¼Œå®ç°KæŠ˜äº¤å‰éªŒè¯çš„æ•°æ®åˆ’åˆ†

**ä»»åŠ¡**ï¼šå°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºKç»„ï¼Œä»¥ä¾¿è¿›è¡ŒKæŠ˜äº¤å‰éªŒè¯ã€‚æ¯ä¸€æŠ˜ä¸­ä½¿ç”¨K-1ä»½æ•°æ®ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä½™1ä»½ä½œä¸ºéªŒè¯é›†ã€‚

**KæŠ˜äº¤å‰éªŒè¯åŸç†**ï¼š

å°†æ•°æ®é›†åˆ’åˆ†ä¸ºKä¸ªå¤§å°ç›¸è¿‘çš„äº’æ–¥å­é›†ï¼Œç„¶åè¿›è¡ŒKæ¬¡è®­ç»ƒå’ŒéªŒè¯ï¼š
- ç¬¬1æŠ˜ï¼šä½¿ç”¨ç¬¬1ä»½æ•°æ®ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™K-1ä»½ä½œä¸ºè®­ç»ƒé›†
- ç¬¬2æŠ˜ï¼šä½¿ç”¨ç¬¬2ä»½æ•°æ®ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™K-1ä»½ä½œä¸ºè®­ç»ƒé›†
- ...
- ç¬¬KæŠ˜ï¼šä½¿ç”¨ç¬¬Kä»½æ•°æ®ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™K-1ä»½ä½œä¸ºè®­ç»ƒé›†

æœ€ç»ˆï¼Œæ¯ä¸ªæ ·æœ¬éƒ½è¢«ç”¨ä½œéªŒè¯é›†æ°å¥½ä¸€æ¬¡ï¼Œè¢«ç”¨ä½œè®­ç»ƒé›†K-1æ¬¡ã€‚

**ä»£ç å®ç°**ï¼š

```python
# è®¡ç®—æ¯æŠ˜çš„å¤§å°
fold_size = num_instances // K

# å¯¹æ¯ä¸€æŠ˜
for i in range(K):
    # è®¡ç®—å½“å‰æŠ˜çš„éªŒè¯é›†ç´¢å¼•èŒƒå›´
    valid_start = i * fold_size
    valid_end = (i + 1) * fold_size if i < K - 1 else num_instances
    
    # åˆ›å»ºéªŒè¯é›†ç´¢å¼•
    valid_indices = list(range(valid_start, valid_end))
    
    # åˆ›å»ºè®­ç»ƒé›†ç´¢å¼•ï¼ˆé™¤äº†éªŒè¯é›†ä¹‹å¤–çš„æ‰€æœ‰ç´¢å¼•ï¼‰
    train_indices = list(range(0, valid_start)) + list(range(valid_end, num_instances))
    
    # åˆ’åˆ†æ•°æ®
    X_train_list.append(X[train_indices])
    y_train_list.append(y[train_indices])
    X_valid_list.append(X[valid_indices])
    y_valid_list.append(y[valid_indices])

return X_train_list, y_train_list, X_valid_list, y_valid_list
```

**å…³é”®ç‚¹**ï¼š

1. **æ•°æ®åˆ’åˆ†**ï¼š
   - æ¯æŠ˜çš„å¤§å°ä¸º `fold_size = n // K`
   - æœ€åä¸€æŠ˜åŒ…å«æ‰€æœ‰å‰©ä½™æ ·æœ¬ï¼Œå¤„ç† n ä¸èƒ½è¢« K æ•´é™¤çš„æƒ…å†µ

2. **ç´¢å¼•æ„å»º**ï¼š
   - éªŒè¯é›†ç´¢å¼•ï¼šè¿ç»­çš„ä¸€æ®µåŒºé—´ `[valid_start, valid_end)`
   - è®­ç»ƒé›†ç´¢å¼•ï¼šéªŒè¯é›†ä¹‹å¤–çš„æ‰€æœ‰ç´¢å¼•

3. **è¿”å›å€¼**ï¼š
   - 4ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«Kä¸ªæ•°ç»„
   - `X_train_list[i]` å’Œ `X_valid_list[i]` å¯¹åº”ç¬¬iæŠ˜çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†

**KæŠ˜äº¤å‰éªŒè¯çš„ä¼˜åŠ¿**ï¼š

1. **å……åˆ†åˆ©ç”¨æ•°æ®**ï¼šæ¯ä¸ªæ ·æœ¬éƒ½è¢«ç”¨ä½œéªŒè¯é›†ä¸€æ¬¡ï¼Œé¿å…æ•°æ®æµªè´¹
2. **ç»“æœæ›´å¯é **ï¼šé€šè¿‡Kæ¬¡å®éªŒçš„å¹³å‡æ€§èƒ½è¯„ä¼°æ¨¡å‹ï¼Œå‡å°‘éšæœºæ€§å½±å“
3. **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šä¸åœ¨æµ‹è¯•é›†ä¸Šé€‰æ‹©å‚æ•°ï¼Œä¿è¯æµ‹è¯•é›†çš„ç‹¬ç«‹æ€§
4. **é€‚ç”¨äºå°æ•°æ®é›†**ï¼šå½“æ•°æ®é‡æœ‰é™æ—¶ï¼ŒKæŠ˜äº¤å‰éªŒè¯èƒ½æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½

**å…¸å‹Kå€¼é€‰æ‹©**ï¼š
- K=5 æˆ– K=10 æ˜¯æœ€å¸¸ç”¨çš„é€‰æ‹©
- Kå€¼è¶Šå¤§ï¼Œè®­ç»ƒé›†è¶Šå¤§ï¼Œä½†è®¡ç®—ä»£ä»·è¶Šé«˜
- æç«¯æƒ…å†µ K=n ç§°ä¸ºç•™ä¸€äº¤å‰éªŒè¯ï¼ˆLOOCVï¼‰ï¼Œè®¡ç®—ä»£ä»·å¾ˆå¤§

#### 2.4.2 è¡¥å…¨å‡½æ•°K_fold_cross_validationï¼Œå®ç°è¶…å‚æ•°æœç´¢

**ä»»åŠ¡**ï¼šä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯æœç´¢æœ€ä¼˜è¶…å‚æ•°ç»„åˆï¼Œè¯„ä¼°ä¸åŒè¶…å‚æ•°ä¸‹æ¨¡å‹çš„æ€§èƒ½ã€‚

**è¶…å‚æ•°æœç´¢ç©ºé—´**ï¼š
- å­¦ä¹ ç‡ï¼š\( \eta \in \{0.05, 0.04, 0.03, 0.02, 0.01\} \)
- æ­£åˆ™åŒ–ç³»æ•°ï¼š\( \lambda \in \{10^{-7}, 10^{-5}, 10^{-3}, 10^{-1}, 1, 10, 100\} \)
- å…± 5 Ã— 7 = 35 ä¸ªè¶…å‚æ•°ç»„åˆ
- K=5 æŠ˜äº¤å‰éªŒè¯ï¼Œæ€»è®¡è®­ç»ƒ 35 Ã— 5 = 175 æ¬¡

**ä»£ç å®ç°**ï¼š

```python
best_avg_error = float('inf')

# éå†æ‰€æœ‰è¶…å‚æ•°ç»„åˆ
for alpha in alphas:
    for lambda_reg in lambdas:
        # å­˜å‚¨æ¯ä¸€æŠ˜çš„éªŒè¯è¯¯å·®
        fold_errors = []
        
        # KæŠ˜äº¤å‰éªŒè¯
        for k in range(K):
            # è·å–ç¬¬kæŠ˜çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†
            X_train_k = X_train_list[k]
            y_train_k = y_train_list[k]
            X_valid_k = X_valid_list[k]
            y_valid_k = y_valid_list[k]
            
            # åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹
            theta_hist, loss_hist = grad_descent(
                X_train_k, y_train_k,
                lambda_reg=lambda_reg,
                alpha=alpha,
                num_iter=num_iter,
                check_gradient=False
            )
            
            # è·å–æœ€ç»ˆå‚æ•°
            theta_final = theta_hist[-1]
            
            # åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å‡æ–¹è¯¯å·®ï¼ˆä¸å¸¦æ­£åˆ™åŒ–é¡¹ï¼‰
            predictions = np.dot(X_valid_k, theta_final)
            mse = np.mean((predictions - y_valid_k) ** 2)
            fold_errors.append(mse)
        
        # è®¡ç®—KæŠ˜çš„å¹³å‡éªŒè¯è¯¯å·®
        avg_error = np.mean(fold_errors)
        
        # æ›´æ–°æœ€ä½³è¶…å‚æ•°
        if avg_error < best_avg_error:
            best_avg_error = avg_error
            alpha_best = alpha
            lambda_best = lambda_reg

return alpha_best, lambda_best
```

**å…³é”®ç‚¹**ï¼š

1. **éªŒè¯æŒ‡æ ‡**ï¼šä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è¯„ä¼°æ¨¡å‹ï¼Œä¸åŒ…å«æ­£åˆ™åŒ–é¡¹
   \[
   \text{MSE} = \frac{1}{m}\sum_{i=1}^m (h_\theta(x_i) - y_i)^2
   \]

2. **æœç´¢ç­–ç•¥**ï¼šç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰ï¼Œéå†æ‰€æœ‰è¶…å‚æ•°ç»„åˆ

3. **æ¨¡å‹é€‰æ‹©**ï¼šé€‰æ‹©KæŠ˜å¹³å‡éªŒè¯è¯¯å·®æœ€å°çš„è¶…å‚æ•°ç»„åˆ

**å®éªŒè„šæœ¬**ï¼š
```bash
python experiment_hyperparameter.py
```

**å®éªŒç»“æœ**ï¼š

ä¸åŒè¶…å‚æ•°ç»„åˆä¸‹çš„éªŒè¯é›†å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š

| Î» \ Î· | 0.01 | 0.02 | 0.03 | 0.04 | 0.05 |
|:---:|:---:|:---:|:---:|:---:|:---:|
| 1e-07 | 3.202 | 2.955 | 2.944 | 2.963 | 2.988 |
| 1e-05 | 3.202 | 2.955 | **2.944** | 2.963 | 2.987 |
| 0.001 | 3.213 | 2.960 | **2.942** â­ | 2.955 | 2.974 |
| 0.1 | 4.337 | 4.272 | 4.269 | 4.269 | 4.269 |
| 1.0 | 6.321 | 6.321 | 6.321 | 6.321 | 6.321 |
| 10 | 6.814 | 6.814 | 6.814 | 2.2Ã—10Â¹â¸â¹ | inf |
| 100 | 8.9Ã—10Â²â¶â´ | NaN | NaN | NaN | NaN |

*â­æ ‡è®°æœ€ä¼˜ç»„åˆ*

**å¯è§†åŒ–ç»“æœ**ï¼š

![è¶…å‚æ•°æœç´¢çƒ­åŠ›å›¾](sgd/hyperparameter_heatmap.png)

*å›¾1ï¼šè¶…å‚æ•°æœç´¢çƒ­åŠ›å›¾ã€‚é¢œè‰²è¶Šç»¿è¡¨ç¤ºMSEè¶Šå°ï¼Œçº¢æ˜Ÿæ ‡è®°æœ€ä¼˜ç»„åˆ*

![å­¦ä¹ ç‡å¯¹æ¯”æ›²çº¿](sgd/learning_rate_comparison.png)

*å›¾2ï¼šä¸åŒæ­£åˆ™åŒ–ç³»æ•°ä¸‹å­¦ä¹ ç‡å¯¹MSEçš„å½±å“*

**æœ€ä¼˜è¶…å‚æ•°ä¸æµ‹è¯•é›†æ€§èƒ½**ï¼š

| æŒ‡æ ‡ | æ•°å€¼ |
|:---:|:---:|
| æœ€ä¼˜å­¦ä¹ ç‡ \( \eta^* \) | 0.03 |
| æœ€ä¼˜æ­£åˆ™åŒ–ç³»æ•° \( \lambda^* \) | 0.001 |
| äº¤å‰éªŒè¯MSE | 2.942 |
| æµ‹è¯•é›†MSE | 2.421 |
| æµ‹è¯•é›†RMSE | 1.556 |

**ä¸»è¦å‘ç°**ï¼š

1. **å­¦ä¹ ç‡å½±å“**ï¼š
   - \( \eta = 0.03 \) æœ€ä¼˜ï¼Œ\( \eta = 0.01 \) è¿‡å°ï¼ˆMSEâ‰ˆ3.20ï¼‰ï¼Œ\( \eta \geq 0.04 \) åœ¨å¤§ \( \lambda \) æ—¶æ˜“å‘æ•£

2. **æ­£åˆ™åŒ–å½±å“**ï¼š
   - \( \lambda = 0.001 \) æœ€ä¼˜ï¼Œ\( \lambda \in \{10^{-7}, 10^{-5}, 10^{-3}\} \) æ€§èƒ½ç›¸è¿‘ï¼ˆ"æ€§èƒ½å¹³å°"ç°è±¡ï¼‰
   - \( \lambda \geq 0.1 \) è¿‡å¼ºå¯¼è‡´æ¬ æ‹Ÿåˆï¼Œ\( \lambda \geq 10 \) æ—¶å¤§å¤šæ•°å­¦ä¹ ç‡ä¸‹å‘æ•£

3. **æ•°å€¼ç¨³å®šæ€§**ï¼š
   - \( \lambda = 10, \eta = 0.04 \) æ—¶æŸå¤±çˆ†ç‚¸è‡³ \( 10^{189} \)
   - \( \lambda = 100 \) æ—¶å‡ ä¹å…¨éƒ¨å‘æ•£ï¼Œè¯´æ˜å¤§æ­£åˆ™åŒ–ç³»æ•°ä¸¥é‡æ¶åŒ–æ•°å€¼ç¨³å®šæ€§

4. **æ³›åŒ–èƒ½åŠ›**ï¼š
   - æµ‹è¯•é›†MSEï¼ˆ2.421ï¼‰ä¼˜äºéªŒè¯é›†MSEï¼ˆ2.942ï¼‰ï¼Œæ¨¡å‹æ³›åŒ–è‰¯å¥½

**è¶…å‚æ•°é€‰æ‹©å¯ç¤º**ï¼š

1. **æœç´¢ç­–ç•¥**ï¼š
   - æ­£åˆ™åŒ–ç³»æ•°åº”åœ¨å¯¹æ•°å°ºåº¦ä¸Šæœç´¢ï¼ˆè·¨è¶Šå¤šä¸ªæ•°é‡çº§ï¼‰
   - æœ¬å®éªŒæœ€ä¼˜åŒºåŸŸåœ¨ \( \eta \in [0.02, 0.03] \)ï¼Œ\( \lambda \in [10^{-7}, 10^{-3}] \)
   - å…±è¿›è¡Œ175æ¬¡è®­ç»ƒï¼ˆ35ä¸ªç»„åˆ Ã— 5æŠ˜ï¼‰

2. **è¶…å‚æ•°äº¤äº’ä½œç”¨**ï¼š
   - å¯¹äºå° \( \lambda \)ï¼ˆ\( 10^{-7} \sim 10^{-3} \)ï¼‰ï¼Œæœ€ä¼˜å­¦ä¹ ç‡å‡ä¸º \( \eta = 0.03 \)
   - è¾ƒå°å­¦ä¹ ç‡ï¼ˆ\( \eta = 0.01 \)ï¼‰æ›´é€‚åˆå‡ ä¹æ— æ­£åˆ™åŒ–çš„æƒ…å†µï¼ˆ\( \lambda = 10^{-7} \)ï¼‰
   - è¾ƒå¤§å­¦ä¹ ç‡éœ€è¦é€‚å½“æ­£åˆ™åŒ–æ¥ç¨³å®šè®­ç»ƒ

3. **å®è·µå»ºè®®**ï¼š
   - ç¨³å®šæ€§ä¼˜å…ˆï¼šæ’é™¤å¯¼è‡´inf/NaNçš„ä¸ç¨³å®šç»„åˆ
   - ä»å°å­¦ä¹ ç‡å’Œå°æ­£åˆ™åŒ–ç³»æ•°å¼€å§‹å°è¯•
   - KæŠ˜äº¤å‰éªŒè¯å¯æ›´å¯é åœ°è¯„ä¼°è¶…å‚æ•°æ€§èƒ½

---

### 2.5 éšæœºæ¢¯åº¦ä¸‹é™

#### 2.5.1 å†™å‡ºå°æ‰¹é‡SGDç›®æ ‡å‡½æ•° \( J_{\text{SGD}}(\theta) \) çš„æ¢¯åº¦è¡¨è¾¾å¼

**é—®é¢˜èƒŒæ™¯**ï¼š

å…¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬è®¡ç®—æ¢¯åº¦ï¼Œè®¡ç®—ä»£ä»·é«˜ã€‚å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆMini-batch SGDï¼‰æ¯æ¬¡åªä½¿ç”¨ \( n \) ä¸ªéšæœºé‡‡æ ·çš„æ ·æœ¬æ¥è¿‘ä¼¼æ¢¯åº¦ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

**ç›®æ ‡å‡½æ•°å®šä¹‰**ï¼š

- æ€»ä½“ç›®æ ‡å‡½æ•°ï¼ˆå…¨æ‰¹é‡ï¼Œå…± \( m \) ä¸ªæ ·æœ¬ï¼‰ï¼š
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x_i) - y_i)^2 + \lambda \|\theta\|^2
\]

- å°æ‰¹é‡SGDç›®æ ‡å‡½æ•°ï¼ˆé‡‡æ ·ç´¢å¼•é›† \( \{i_1, i_2, \dots, i_n\} \)ï¼Œå…± \( n \) ä¸ªæ ·æœ¬ï¼‰ï¼š
\[
J_{\text{SGD}}(\theta) = \frac{1}{n} \sum_{j=1}^n (h_\theta(x_{i_j}) - y_{i_j})^2 + \lambda \|\theta\|^2
\]

**æ¢¯åº¦æ¨å¯¼**ï¼š

å¯¹ \( J_{\text{SGD}}(\theta) \) ä¸­çš„ä¸¤é¡¹åˆ†åˆ«æ±‚æ¢¯åº¦ï¼š

1. å¯¹å‡æ–¹è¯¯å·®é¡¹ï¼š
\[
\frac{\partial}{\partial\theta}\left[\frac{1}{n} \sum_{j=1}^n (h_\theta(x_{i_j}) - y_{i_j})^2\right] = \frac{2}{n} \sum_{j=1}^n (h_\theta(x_{i_j}) - y_{i_j}) x_{i_j}
\]

2. å¯¹æ­£åˆ™åŒ–é¡¹ï¼š
\[
\frac{\partial}{\partial\theta}\left[\lambda \|\theta\|^2\right] = 2\lambda \theta
\]

**æœ€ç»ˆç­”æ¡ˆ**ï¼š
\[
\nabla J_{\text{SGD}}(\theta) = \frac{2}{n} \sum_{j=1}^n (h_\theta(x_{i_j}) - y_{i_j}) x_{i_j} + 2\lambda \theta
\]

#### 2.5.2 è¯æ˜éšæœºæ¢¯åº¦æ˜¯æ€»ä½“æ¢¯åº¦çš„æ— åä¼°è®¡

**å‘½é¢˜**ï¼šéšæœºæ¢¯åº¦ \( \nabla J_{\text{SGD}}(\theta) \) çš„æœŸæœ›ç­‰äºæ€»ä½“æ¢¯åº¦ \( \nabla J(\theta) \)ã€‚

**è¯æ˜**ï¼š

1. **éšæœºæ¢¯åº¦çš„å®šä¹‰**ï¼š
\[
\nabla J_{\text{SGD}}(\theta) = \frac{2}{n} \sum_{j=1}^n (h_\theta(x_{i_j}) - y_{i_j}) x_{i_j} + 2\lambda \theta
\]

2. **å¯¹éšæœºæ¢¯åº¦å–æœŸæœ›**ï¼š
\[
\mathbb{E}[\nabla J_{\text{SGD}}(\theta)] = \frac{2}{n} \sum_{j=1}^n \mathbb{E}_{i_j}[(h_\theta(x_{i_j}) - y_{i_j}) x_{i_j}] + 2\lambda \theta
\]

3. ç”±äºæ¯ä¸ª \( i_j \) ä» \( \{1, 2, \dots, m\} \) ä¸­å‡åŒ€ç‹¬ç«‹é‡‡æ ·ï¼š
\[
\mathbb{E}_{i_j}[(h_\theta(x_{i_j}) - y_{i_j}) x_{i_j}] = \frac{1}{m} \sum_{i=1}^m (h_\theta(x_i) - y_i) x_i
\]

4. **ä»£å…¥å¹¶åŒ–ç®€**ï¼š
\[
\begin{aligned}
\mathbb{E}[\nabla J_{\text{SGD}}(\theta)] &= \frac{2}{n} \sum_{j=1}^n \left[\frac{1}{m} \sum_{i=1}^m (h_\theta(x_i) - y_i) x_i\right] + 2\lambda \theta \\
&= \frac{2}{n} \cdot n \cdot \frac{1}{m} \sum_{i=1}^m (h_\theta(x_i) - y_i) x_i + 2\lambda \theta \\
&= \frac{2}{m} \sum_{i=1}^m (h_\theta(x_i) - y_i) x_i + 2\lambda \theta \\
&= \nabla J(\theta)
\end{aligned}
\]

**ç»“è®º**ï¼š
\[
\mathbb{E}[\nabla J_{\text{SGD}}(\theta)] = \nabla J(\theta)
\]

å³ï¼š**éšæœºæ¢¯åº¦æ˜¯æ€»ä½“æ¢¯åº¦çš„æ— åä¼°è®¡**ã€‚è¿™æ„å‘³ç€è™½ç„¶å•æ¬¡é‡‡æ ·çš„æ¢¯åº¦å¯èƒ½æœ‰åå·®ï¼Œä½†åœ¨æœŸæœ›æ„ä¹‰ä¸‹ï¼Œéšæœºæ¢¯åº¦æŒ‡å‘ä¸æ€»ä½“æ¢¯åº¦ç›¸åŒçš„æ–¹å‘ã€‚

#### 2.5.3 è¡¥å…¨å‡½æ•°stochastic_grad_descentï¼Œå®ç°éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•

**ç®—æ³•æè¿°**ï¼š

éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descent, SGDï¼‰æ˜¯æ¢¯åº¦ä¸‹é™çš„ä¸€ä¸ªå˜ä½“ï¼Œæ¯æ¬¡è¿­ä»£åªä½¿ç”¨ä¸€å°æ‰¹ï¼ˆmini-batchï¼‰æ ·æœ¬æ¥è®¡ç®—æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ã€‚

**ç®—æ³•æµç¨‹**ï¼š

1. åˆå§‹åŒ–å‚æ•° \( \theta = 0 \)
2. å¯¹äºæ¯æ¬¡è¿­ä»£ \( t = 1, 2, \dots, T \)ï¼š
   - ä»è®­ç»ƒé›†ä¸­éšæœºé‡‡æ · \( n \) ä¸ªæ ·æœ¬ï¼ˆbatch_size = \( n \)ï¼‰
   - ä½¿ç”¨è¿™ \( n \) ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦ \( \nabla J_{\text{SGD}}(\theta) \)
   - æ›´æ–°å‚æ•°ï¼š\( \theta := \theta - \eta \nabla J_{\text{SGD}}(\theta) \)
   - è®°å½•è®­ç»ƒæŸå¤±å’ŒéªŒè¯è¯¯å·®

**ä»£ç å®ç°**ï¼š

```python
for i in range(num_iter):
    # éšæœºé‡‡æ ·batch_sizeä¸ªæ ·æœ¬çš„ç´¢å¼•
    batch_indices = np.random.choice(num_instances, batch_size, replace=False)
    
    # è·å–å°æ‰¹é‡æ•°æ®
    X_batch = X_train[batch_indices]
    y_batch = y_train[batch_indices]
    
    # è®¡ç®—å°æ‰¹é‡ä¸Šçš„æŸå¤±ï¼ˆå¸¦æ­£åˆ™åŒ–ï¼‰
    loss_hist[i] = compute_regularized_square_loss(X_batch, y_batch, theta, lambda_reg)
    
    # è®¡ç®—å°æ‰¹é‡ä¸Šçš„æ¢¯åº¦
    grad = compute_regularized_square_loss_gradient(X_batch, y_batch, theta, lambda_reg)
    
    # æ›´æ–°å‚æ•°ï¼šÎ¸ := Î¸ - Î± * âˆ‡J_SGD(Î¸)
    theta = theta - alpha * grad
    
    # ä¿å­˜å‚æ•°å†å²
    theta_hist[i + 1] = theta
    
    # åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å‡æ–¹è¯¯å·®ï¼ˆä¸å¸¦æ­£åˆ™åŒ–é¡¹ï¼‰
    val_predictions = np.dot(X_val, theta)
    validation_hist[i] = np.mean((val_predictions - y_val) ** 2)

return theta_hist, loss_hist, validation_hist
```

**å…³é”®ç‚¹**ï¼š

1. **éšæœºé‡‡æ ·**ï¼šæ¯æ¬¡è¿­ä»£ä½¿ç”¨ `np.random.choice` éšæœºé€‰æ‹© batch_size ä¸ªæ ·æœ¬
   - `replace=False` ç¡®ä¿åŒä¸€æ‰¹æ¬¡å†…æ ·æœ¬ä¸é‡å¤

2. **æ¢¯åº¦è®¡ç®—**ï¼šä½¿ç”¨å°æ‰¹é‡æ•°æ®è®¡ç®—æ¢¯åº¦
   - æ¢¯åº¦å…¬å¼ï¼š\( \nabla J_{\text{SGD}}(\theta) = \frac{2}{n} \sum_{j=1}^n (h_\theta(x_{i_j}) - y_{i_j}) x_{i_j} + 2\lambda \theta \)

3. **æŸå¤±è®°å½•**ï¼š
   - `loss_hist`ï¼šè®°å½•å°æ‰¹é‡ä¸Šçš„æ­£åˆ™åŒ–æŸå¤±ï¼ˆç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹ï¼‰
   - `validation_hist`ï¼šè®°å½•éªŒè¯é›†ä¸Šçš„MSEï¼ˆä¸å«æ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºè¯„ä¼°æ³›åŒ–æ€§èƒ½ï¼‰

#### 2.5.4 æ‰¹å¤§å°å¯¹SGDæ”¶æ•›æ€§èƒ½çš„å½±å“å®éªŒ

**å®éªŒç›®çš„**ï¼šç ”ç©¶ä¸åŒæ‰¹å¤§å°ï¼ˆbatch_sizeï¼‰å¯¹éšæœºæ¢¯åº¦ä¸‹é™æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§çš„å½±å“ã€‚

**å®éªŒè®¾ç½®**ï¼š

- å›ºå®šæ­£åˆ™åŒ–ç³»æ•°ï¼š\( \lambda = 0 \)
- å­¦ä¹ ç‡ï¼š\( \eta = 0.03 \)ï¼ˆæ ¹æ®2.4.2å°èŠ‚çš„ç»“æœé€‰æ‹©ï¼‰
- æ‰¹å¤§å°ï¼š\( \{1, 4, 8, 16, 32, 64, 160\} \)ï¼ˆ160ä¸ºå…¨æ‰¹é‡ï¼‰
- è¿­ä»£æ¬¡æ•°ï¼š1000æ¬¡
- æ•°æ®åˆ’åˆ†ï¼šä½¿ç”¨ `split_data` å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ80%ï¼‰å’ŒéªŒè¯é›†ï¼ˆ20%ï¼‰

**è¯„ä¼°æŒ‡æ ‡**ï¼š
- è®­ç»ƒæŸå¤±ï¼ˆå°æ‰¹é‡ï¼‰ï¼šæœ‰å™ªå£°ï¼Œä»…ç”¨äºè§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹
- éªŒè¯é›†MSEï¼šä¸å«æ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½
- éªŒè¯é›†æŸå¤±ï¼šå«æ­£åˆ™åŒ–é¡¹ï¼Œç”¨äºåˆ¤æ–­æ”¶æ•›

**å®éªŒè„šæœ¬**ï¼š
```bash
python experiment_batch_size.py
```

**å®éªŒç»“æœ**ï¼š

![æ‰¹å¤§å°å¯¹æ¯”å®éªŒç»“æœ](sgd/batch_size_comparison.png)

*å›¾ï¼šæ‰¹å¤§å°å¯¹SGDæ”¶æ•›æ€§èƒ½çš„å½±å“ã€‚å·¦ä¸Šï¼šè®­ç»ƒæŸå¤±ï¼ˆæœ‰å™ªå£°ï¼‰ï¼›å³ä¸Šï¼šéªŒè¯é›†MSEï¼›å·¦ä¸‹ï¼šéªŒè¯é›†æŸå¤±ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰ï¼›å³ä¸‹ï¼šå‰200æ¬¡è¿­ä»£çš„æ”¶æ•›é€Ÿåº¦å¯¹æ¯”*

**å®éªŒç»“æœæ•°æ®**ï¼š

| æ‰¹å¤§å° | æœ€ç»ˆéªŒè¯MSE | æœ€ç»ˆéªŒè¯æŸå¤± | å‰100æ¬¡è¿­ä»£æŸå¤±ä¸‹é™ | è®­ç»ƒæŸå¤±å™ªå£°ï¼ˆæ ‡å‡†å·®ï¼‰ |
|:---:|:---:|:---:|:---:|:---:|
| 1 | 5.334 | 5.794 | -974.0% | 105.3 |
| 4 | 2.599 | 2.858 | 33.0% | 2.62 |
| 8 | **2.443** â­ | **2.444** | -13.1% | 1.65 |
| 16 | 2.612 | 2.933 | **34.8%** ğŸš€ | 0.87 |
| 32 | 2.552 | 2.511 | 33.9% | 0.59 |
| 64 | 2.681 | 2.491 | 25.2% | 0.34 |
| 160 (å…¨æ‰¹é‡) | 2.555 | 2.555 | 33.7% | 0 |

*â­æœ€ä¼˜æ€§èƒ½ ğŸš€æœ€å¿«æ”¶æ•›ï¼›batch_size=1å‘ç”Ÿæ•°å€¼çˆ†ç‚¸ï¼ˆå‘æ•£ï¼‰*

**ä¸»è¦å‘ç°**ï¼š

1. **è®­ç»ƒæŸå¤±å™ªå£°ä¸æ‰¹å¤§å°çš„å…³ç³»**ï¼š
   - å™ªå£°æ ‡å‡†å·®éšbatch_sizeå¢å¤§è€Œ**æ˜¾è‘—å‡å°**ï¼Œç¬¦åˆç†è®ºï¼ˆâˆ 1/âˆšbatch_sizeï¼‰ï¼š
     - batch_size = 1: å™ªå£° 105.3ï¼ˆæå¤§ï¼‰
     - batch_size = 4: å™ªå£° 2.62ï¼ˆå‡å°‘97.5%ï¼‰
     - batch_size = 8: å™ªå£° 1.65
     - batch_size = 16: å™ªå£° 0.87
     - batch_size = 32: å™ªå£° 0.59
     - batch_size = 64: å™ªå£° 0.34
     - batch_size = 160: å™ªå£° 0ï¼ˆå…¨æ‰¹é‡æ— å™ªå£°ï¼‰

2. **æœ€ä¼˜æ‰¹å¤§å°çš„å‘ç°**ï¼š
   - **batch_size = 8 æ€§èƒ½æœ€ä½³**ï¼šéªŒè¯MSE = 2.443ï¼ˆæœ€ä½ï¼‰
   - **batch_size = 16 æ”¶æ•›æœ€å¿«**ï¼šå‰100æ¬¡è¿­ä»£æŸå¤±ä¸‹é™34.8%
   - **batch_size = 4-64** éƒ½èƒ½è¾¾åˆ°ç›¸è¿‘çš„æœ€ç»ˆæ€§èƒ½ï¼ˆMSE â‰ˆ 2.5-2.7ï¼‰
   - å…¨æ‰¹é‡ï¼ˆ160ï¼‰è¡¨ç°è‰¯å¥½ä½†éæœ€ä¼˜ï¼ˆMSE = 2.555ï¼‰

3. **Mini-batch SGDçš„ä¼˜åŠ¿**ï¼š
   - **é€‚å½“çš„æ‰¹å¤§å°**ï¼ˆ8-32ï¼‰åœ¨Î·=0.03ä¸‹è¡¨ç°ä¼˜äºå…¨æ‰¹é‡
   - batch_size=8 åœ¨**æœ€ç»ˆæ€§èƒ½**ä¸Šè¶…è¿‡å…¨æ‰¹é‡ï¼ˆ2.443 vs 2.555ï¼‰
   - è¿™éªŒè¯äº†SGDçš„æ³›åŒ–ä¼˜åŠ¿ï¼šé€‚åº¦çš„æ¢¯åº¦å™ªå£°æœ‰åŠ©äºé¿å…è¿‡æ‹Ÿåˆ

