# 作业1:线性模型和支持向量机

**姓名**: 杨憬晗  
**学号**: 2022010808

---

## 线性模型与梯度下降

### 2.1 特征归一化

在实际任务中，若各维特征的量级差异较大，梯度下降的收敛会显著变慢；同时，在使用正则化时，量级较大的特征对正则项影响更强。因此需要进行特征归一化。常用做法是在训练集上对每个特征进行仿射变换，将其映射到区间[0,1]；并对测试集施加与训练集一致的变换。

#### 2.1.1 补全函数split_data，将数据集划分为训练集与测试集

**实现思路**：
- 根据`split_size`中的比例，依次计算每个划分的样本数量
- 使用索引切片将数据集划分为多个部分
- 处理最后一个划分以确保包含所有剩余样本

**代码实现**：
```python
start_idx = 0
for ratio in split_size:
    # 计算当前划分的样本数量
    split_num = int(num_instances * ratio)
    end_idx = start_idx + split_num
    
    # 处理最后一个划分，确保包含所有剩余样本
    if end_idx > num_instances or ratio == split_size[-1]:
        end_idx = num_instances
    
    # 划分数据
    X_list.append(X[start_idx:end_idx])
    y_list.append(y[start_idx:end_idx])
    
    start_idx = end_idx

return X_list, y_list
```

**关键点**：
- 使用累积索引方式进行数据划分
- 最后一个划分需要包含所有剩余样本，避免因浮点数计算导致的样本丢失

#### 2.1.2 补全函数feature_normalization，实现特征归一化

**实现思路**：
- 在训练集上计算每个特征的最小值和最大值
- 使用Min-Max归一化公式: \( x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}} \)
- 对测试集使用训练集的最小值和最大值进行相同的变换

**代码实现**：
```python
# 从训练集计算每个特征的最小值和最大值
train_min = np.min(train, axis=0)
train_max = np.max(train, axis=0)

# 避免除零错误：如果最大值等于最小值，说明该特征为常数
# 此时将范围设为1，保持归一化后的值不变
range_vals = train_max - train_min
range_vals[range_vals == 0] = 1  # 避免除零

# 将训练集归一化到 [0, 1]
train_normalized = (train - train_min) / range_vals

# 对测试集使用训练集的最小值和最大值进行相同的变换
test_normalized = (test - train_min) / range_vals

return train_normalized, test_normalized
```

**关键点**：
1. **统计量的计算**：所有归一化参数（最小值、最大值）都从训练集计算得出
2. **测试集一致性**：测试集必须使用训练集的统计量进行变换，确保分布一致
3. **边界情况处理**：当某个特征的最大值等于最小值时（常数特征），设置范围为1以避免除零错误
4. **向量化操作**：使用numpy的广播机制，对所有特征同时进行归一化，提高效率

**理论依据**：
- Min-Max归一化将特征线性映射到[0,1]区间
- 通过仿射变换 \( x' = \frac{x - min}{max - min} \) 实现
- 保证所有特征处于相同量级，有利于梯度下降的收敛
- 在正则化时，避免量级大的特征主导正则项

---

### 2.2 目标函数与梯度

岭回归(Ridge Regression)是在线性回归的基础上加入L2正则化，目的是防止过拟合并提高模型的泛化能力。

#### 2.2.1 写出J(θ)的矩阵形式表达式

**已知条件**：
- 设计矩阵：\( X \in \mathbb{R}^{m \times (d+1)} \)，其中第 \( i \) 行为 \( x_i^\top \)
- 标签向量：\( y \in \mathbb{R}^m \)
- 参数向量：\( \theta \in \mathbb{R}^{d+1} \)
- 正则化系数：\( \lambda > 0 \)

**目标函数**：

岭回归的目标函数为：
\[
J(\theta) = \frac{1}{m} \| X\theta - y \|^2 + \lambda \| \theta \|^2
\]

展开为矩阵形式：
\[
J(\theta) = \frac{1}{m}(X\theta - y)^\top (X\theta - y) + \lambda \theta^\top \theta
\]

其中：
- 第一项 \( \frac{1}{m}(X\theta - y)^\top (X\theta - y) \) 是均方误差损失
- 第二项 \( \lambda \theta^\top \theta \) 是L2正则化项

#### 2.2.2 补全函数compute_regularized_square_loss，计算J(θ)

**实现目标**：根据 \( J(\theta) = \frac{1}{m}(X\theta - y)^\top (X\theta - y) + \lambda \theta^\top \theta \) 实现目标函数。

**代码实现**：
```python
num_instances = X.shape[0]

# 计算预测值
predictions = np.dot(X, theta)

# 计算残差
residuals = predictions - y

# 计算平方损失: (1/m) * ||Xθ - y||²
square_loss = np.dot(residuals, residuals) / num_instances

# 计算正则化项: λ * ||θ||²
regularization = lambda_reg * np.dot(theta, theta)

# 目标函数 J(θ)
loss = square_loss + regularization

return loss
```

**关键点**：
- 使用 `np.dot` 进行向量化计算

#### 2.2.3 推导J(θ)的梯度

**推导过程**：

对 \( J(\theta) = \frac{1}{m}(X\theta - y)^\top(X\theta - y) + \lambda \theta^\top\theta \) 求梯度。

分别对两项求导：

1. **对均方误差项求导**：
   \[
   \frac{\partial}{\partial\theta}\left[\frac{1}{m}(X\theta - y)^\top(X\theta - y)\right] = \frac{2}{m}X^\top(X\theta - y)
   \]

2. **对正则化项求导**：
   \[
   \frac{\partial}{\partial\theta}\left[\lambda\theta^\top\theta\right] = 2\lambda\theta
   \]

**梯度表达式**：
\[
\nabla J(\theta) = \frac{2}{m}X^\top(X\theta - y) + 2\lambda\theta
\]

**梯度含义**：
- \( \frac{2}{m}X^\top(X\theta - y) \)：均方误差项的梯度，将残差投影到特征空间
- \( 2\lambda\theta \)：正则化项的梯度，使参数向零收缩，防止过拟合
- 梯度下降更新：\( \theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t) \)

#### 2.2.4 补全函数compute_regularized_square_loss_gradient，实现J(θ)的梯度

**实现目标**： \( \nabla J(\theta) = \frac{2}{m}X^\top(X\theta - y) + 2\lambda\theta \)

**代码实现**：
```python
num_instances = X.shape[0]

# 计算预测值
predictions = np.dot(X, theta)

# 计算残差
residuals = predictions - y

# 计算梯度: (2/m) * X^T(Xθ - y) + 2λθ
grad = 2 * np.dot(X.T, residuals) / num_instances + 2 * lambda_reg * theta

return grad
```

**关键点**：
- 使用 `X.T` 进行矩阵转置
- 向量化实现，无需循环

